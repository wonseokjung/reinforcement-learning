{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#라이브러리 임포트\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Policy evaluation을 먼저 만들어준다. \n",
    "\n",
    "def policy_eval(policy,env,discount_factor=1.0, theta=0.00001):\n",
    "    \n",
    "    V=np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        delta=0\n",
    "        for s in range(env.nS):\n",
    "            v=0\n",
    "            for a, action_prob in enumerate(policy[s]):#인덱스와 내용 출력\n",
    "                \n",
    "                for prob,next_state,reward,done in env.P[s][a]:\n",
    "                    v+= action_prob*prob*(reward+discount_factor*V[next_state])\n",
    "            delta=max(delta, np.abs(v-V[s]))\n",
    "            V[s]=v\n",
    "            \n",
    "        if delta<theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy 개선하기\n",
    "\n",
    "\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "     Policy Improvement 알고리즘. \n",
    "     최적 폴리시를 찾을때까지 폴리시를 관찰하고 개선한다. \n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI 환경\n",
    "        policy_eval_fn: 폴리시 이벨류에이션 함수는 3개의 아규먼트를 받는다. \n",
    "        1) 폴리시 2) 환경 3)디스카운트펙터\n",
    "        discount_factor: Lambda discount factor.\n",
    "        \n",
    "    Returns:1\n",
    "        폴리시와 벨류를 포함한 터플을 리턴\n",
    "        폴리시는 최적화된 폴리시이다. 각각 스테이트의 [스테이트, 엑션]의 모양이다. \n",
    "        엑션의 확률분포가 포함되어있다.\n",
    "        V 는 최적 폴리시를 위한 벨류함수이다.\n",
    "        \n",
    "    \"\"\"\n",
    "    # 랜덤 폴리시로 시작\n",
    "    policy = np.ones([env.nS,env.nA])/env.nA #각 엑션을 0.25로 \n",
    "    while True: \n",
    "        # 현재 policy evaluate \n",
    "        \n",
    "        V= policy_eval_fn(policy,env,discount_factor)\n",
    "        \n",
    "        \n",
    "        #폴리시에 변화를 주면 false로 설정된다. \n",
    "        policy_stable = True\n",
    "\n",
    "        \n",
    "        \n",
    "        #각 스테이트마다.. \n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            #폴리시를 가장 크게 하는 값을 찾는다\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            #한 스텝 다음에서의 베스트엑션을 찾는다. \n",
    "        \n",
    "                \n",
    "            action_values = np.zeros(env.nA) #엑션만큼의 값이 0인 어레이를 생성함\n",
    "            \n",
    "            #각 스테이트에서 할수 있는 엑션에서 \n",
    "            for a in range(env.nA):\n",
    "                #엑션의 확률과, 엑션을 했을때 갈수 있는 다음스테이트, 리워드, 그리고 끝 ( env.P[s][a] 안에서 ) \n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    \n",
    "                    #엑션의 가치를 구한다. \n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                    \n",
    "                    #np.argmax를 사용하여 action_values\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "                \n",
    "            # 2-D array with ones on the diagonal and zeros 4*4인  ones on the diagonal and zeros elsewher를 만들어주고,\n",
    "            \n",
    "            #해주고 베스트엑션을 리턴한다. \n",
    "            \n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        \n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 ms ± 852 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1000\n",
    "policy, v = policy_improvement(env)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 ns ± 0.567 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
